{"cells":[{"cell_type":"markdown","id":"1df29eda","metadata":{"id":"1df29eda"},"source":["Step 0. Unzip enron1.zip into the current directory."]},{"cell_type":"markdown","id":"bf32cfce","metadata":{"id":"bf32cfce"},"source":["Step 1. Traverse the dataset and create a Pandas dataframe. This is already done for you and should run without any errors. You should recognize Pandas from task 1."]},{"cell_type":"code","execution_count":25,"id":"20c5d195","metadata":{"id":"20c5d195"},"outputs":[],"source":["# import pandas as pd\n","# import os\n","\n","# def read_category(category, directory):\n","#     emails = []\n","#     for filename in os.listdir(directory):\n","#         if not filename.endswith(\".txt\"):\n","#             continue\n","#         with open(os.path.join(directory, filename), 'r') as fp:\n","#             try:\n","#                 content = fp.read()\n","#                 emails.append({'name': filename, 'content': content, 'category': category})\n","#             except:\n","#                 print(f'skipped {filename}')\n","#     return emails\n","\n","# def read_spam():\n","#     category = 'spam'\n","#     directory = './enron1/spam'\n","#     return read_category(category, directory)\n","\n","# def read_ham():\n","#     category = 'ham'\n","#     directory = './enron1/ham'\n","#     return read_category(category, directory)\n","\n","\n","# ham = read_ham()\n","# spam = read_spam()\n","\n","# df = pd.DataFrame.from_records(ham)\n","# df = df.append(pd.DataFrame.from_records(spam))"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","def read_category(category, directory):\n","    emails = []\n","    for filename in os.listdir(directory):\n","        if filename.endswith(\".txt\"):\n","            try:\n","                with open(os.path.join(directory, filename), 'r', encoding='utf-8', errors='ignore') as fp:\n","                    content = fp.read()\n","                    emails.append({'name': filename, 'content': content, 'category': category})\n","            except Exception as e:\n","                print(f\"skipped {filename} due to error: {e}\")\n","    return emails\n","\n","def read_spam():\n","    category = 'spam'\n","    directory = './enron1/spam'\n","    return read_category(category, directory)\n","\n","def read_ham():\n","    category = 'ham'\n","    directory = './enron1/ham'\n","    return read_category(category, directory)\n","\n","ham = read_ham()\n","spam = read_spam()\n","\n","# Use different variable names to avoid overwriting\n","df_ham = pd.DataFrame.from_records(ham)\n","df_spam = pd.DataFrame.from_records(spam)\n","# df_spam.head(5)\n","\n","# Combine DataFrames\n","df_combined = pd.concat([df_ham, df_spam], ignore_index=True)\n"]},{"cell_type":"markdown","id":"1a1c23fd","metadata":{"id":"1a1c23fd"},"source":["Step 2. Data cleaning is a critical part of machine learning. You and I can recognize that 'Hello' and 'hello' are the same word but a machine does not know this a priori. Therefore, we can 'help' the machine by conducting such normalization steps for it. Write a function `preprocessor` that takes in a string and replaces all non alphabet characters with a space and then lowercases the result."]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>content</th>\n","      <th>category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0001.1999-12-10.farmer.ham.txt</td>\n","      <td>Subject: christmas tree farm pictures\\n</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0002.1999-12-13.farmer.ham.txt</td>\n","      <td>Subject: vastar resources , inc .\\ngary , prod...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0003.1999-12-14.farmer.ham.txt</td>\n","      <td>Subject: calpine daily gas nomination\\n- calpi...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0004.1999-12-14.farmer.ham.txt</td>\n","      <td>Subject: re : issue\\nfyi - see note below - al...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0005.1999-12-14.farmer.ham.txt</td>\n","      <td>Subject: meter 7268 nov allocation\\nfyi .\\n- -...</td>\n","      <td>ham</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5167</th>\n","      <td>5163.2005-09-06.GP.spam.txt</td>\n","      <td>Subject: our pro - forma invoice attached\\ndiv...</td>\n","      <td>spam</td>\n","    </tr>\n","    <tr>\n","      <th>5168</th>\n","      <td>5164.2005-09-06.GP.spam.txt</td>\n","      <td>Subject: str _ rndlen ( 2 - 4 ) } { extra _ ti...</td>\n","      <td>spam</td>\n","    </tr>\n","    <tr>\n","      <th>5169</th>\n","      <td>5167.2005-09-06.GP.spam.txt</td>\n","      <td>Subject: check me out !\\n61 bb\\nhey derm\\nbbbb...</td>\n","      <td>spam</td>\n","    </tr>\n","    <tr>\n","      <th>5170</th>\n","      <td>5170.2005-09-06.GP.spam.txt</td>\n","      <td>Subject: hot jobs\\nglobal marketing specialtie...</td>\n","      <td>spam</td>\n","    </tr>\n","    <tr>\n","      <th>5171</th>\n","      <td>5171.2005-09-06.GP.spam.txt</td>\n","      <td>Subject: save up to 89 % on ink + no shipping ...</td>\n","      <td>spam</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5172 rows Ã— 3 columns</p>\n","</div>"],"text/plain":["                                name  \\\n","0     0001.1999-12-10.farmer.ham.txt   \n","1     0002.1999-12-13.farmer.ham.txt   \n","2     0003.1999-12-14.farmer.ham.txt   \n","3     0004.1999-12-14.farmer.ham.txt   \n","4     0005.1999-12-14.farmer.ham.txt   \n","...                              ...   \n","5167     5163.2005-09-06.GP.spam.txt   \n","5168     5164.2005-09-06.GP.spam.txt   \n","5169     5167.2005-09-06.GP.spam.txt   \n","5170     5170.2005-09-06.GP.spam.txt   \n","5171     5171.2005-09-06.GP.spam.txt   \n","\n","                                                content category  \n","0               Subject: christmas tree farm pictures\\n      ham  \n","1     Subject: vastar resources , inc .\\ngary , prod...      ham  \n","2     Subject: calpine daily gas nomination\\n- calpi...      ham  \n","3     Subject: re : issue\\nfyi - see note below - al...      ham  \n","4     Subject: meter 7268 nov allocation\\nfyi .\\n- -...      ham  \n","...                                                 ...      ...  \n","5167  Subject: our pro - forma invoice attached\\ndiv...     spam  \n","5168  Subject: str _ rndlen ( 2 - 4 ) } { extra _ ti...     spam  \n","5169  Subject: check me out !\\n61 bb\\nhey derm\\nbbbb...     spam  \n","5170  Subject: hot jobs\\nglobal marketing specialtie...     spam  \n","5171  Subject: save up to 89 % on ink + no shipping ...     spam  \n","\n","[5172 rows x 3 columns]"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["df=df_combined\n","df"]},{"cell_type":"code","execution_count":40,"id":"c447c901","metadata":{"id":"c447c901"},"outputs":[],"source":["import re\n","\n","def preprocessor(e):\n","    e=re.sub('[^a-zA-Z]', ' ', e)\n","    e=e.lower()\n","    return e"]},{"cell_type":"markdown","id":"ba32521d","metadata":{"id":"ba32521d"},"source":["Step 3. We will now train the machine learning model. All the functions that you will need are imported for you. The instructions explain how the work and hint at which functions to use. You will likely need to refer to the scikit learn documentation to see how exactly to invoke the functions. It will be handy to keep that tab open."]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['ham' 'spam']\n"]}],"source":["print(df['category'].unique())\n"]},{"cell_type":"code","execution_count":42,"id":"1442d377","metadata":{"id":"1442d377"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.98\n","Confusion Matrix:\n","[[732  17]\n"," [  8 278]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","         ham       0.99      0.98      0.98       749\n","        spam       0.94      0.97      0.96       286\n","\n","    accuracy                           0.98      1035\n","   macro avg       0.97      0.97      0.97      1035\n","weighted avg       0.98      0.98      0.98      1035\n","\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Vansh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","# Instantiate a CountVectorizer with the preprocessor\n","vectorizer = CountVectorizer(preprocessor=preprocessor)\n","\n","# Use train_test_split to split the dataset into a train dataset and a test dataset\n","X_train, X_test, y_train, y_test = train_test_split(df['content'], df['category'], test_size=0.2, random_state=42)\n","\n","# Use the vectorizer to transform the existing dataset into a form in which the model can learn from\n","X_train_vectorized = vectorizer.fit_transform(X_train)\n","X_test_vectorized = vectorizer.transform(X_test)\n","\n","# Use the LogisticRegression model to fit to the train dataset\n","model = LogisticRegression()\n","model.fit(X_train_vectorized, y_train)\n","\n","# Validate that the model has learned something\n","X_test_predictions = model.predict(X_test_vectorized)\n","\n","# Evaluate the performance of the model\n","accuracy = accuracy_score(y_test, X_test_predictions)\n","conf_matrix = confusion_matrix(y_test, X_test_predictions)\n","classification_rep = classification_report(y_test, X_test_predictions)\n","\n","print(f\"Accuracy: {accuracy:.2f}\")\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n","print(\"Classification Report:\")\n","print(classification_rep)\n"]},{"cell_type":"markdown","id":"9674d032","metadata":{"id":"9674d032"},"source":["Step 4."]},{"cell_type":"code","execution_count":45,"id":"6b7d78c9","metadata":{"id":"6b7d78c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 10 features:\n","['aa' 'aaa' 'aaas' 'aabda' 'aabvmmq' 'aac' 'aachecar' 'aaer' 'aafco'\n"," 'aaiabe']\n","Error: Feature names and coefficients have different lengths.\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","\n","def preprocessor(text):\n","    # Replace non-alphabetic characters with a space and convert to lowercase\n","    return ''.join([char if char.isalpha() else ' ' for char in text]).lower()\n","\n","# Instantiate a CountVectorizer with the preprocessor\n","vectorizer = CountVectorizer(preprocessor=preprocessor)\n","\n","# Fit and transform the training data\n","X_train_vectorized = vectorizer.fit_transform(df_combined['content'])\n","\n","# Get the feature names (words) created by the vectorizer\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Display the first 10 features\n","print(\"Top 10 features:\")\n","print(feature_names[:10])\n","\n","# Access the coefficients from the logistic regression model\n","coefficients = model.coef_[0]\n","\n","# Ensure feature_names and coefficients have the same length\n","if len(feature_names) == len(coefficients):\n","    # Create a DataFrame to associate feature names with their coefficients\n","    coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n","\n","    # Sort the DataFrame by the magnitude of coefficients in descending order\n","    coefficients_df_sorted = coefficients_df.reindex(coefficients_df['Coefficient'].abs().sort_values(ascending=False).index)\n","\n","    # Display the top 10 positive and negative features\n","    top_positive_features = coefficients_df_sorted.head(10)\n","    top_negative_features = coefficients_df_sorted.tail(10)\n","\n","    print(\"\\nTop 10 Positive Features (Spam):\")\n","    print(top_positive_features)\n","\n","    print(\"\\nTop 10 Negative Features (Ham):\")\n","    print(top_negative_features)\n","else:\n","    print(\"Error: Feature names and coefficients have different lengths.\")\n"]},{"cell_type":"markdown","id":"d267e7ad","metadata":{"id":"d267e7ad"},"source":["Submission\n","1. Upload the jupyter notebook to Forage."]},{"cell_type":"markdown","id":"LI4u_ZUGToDQ","metadata":{"id":"LI4u_ZUGToDQ"},"source":["All Done!"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"task3.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":5}
